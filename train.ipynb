{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from model.CNN import CNN\n",
    "from util.checkpoints import save_checkpoint, load_checkpoint\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "## random seed 고정 (무조건 1순위)\n",
    "## 재현 가능하도록 하기 위해\n",
    "## train dataset과 val dataset을 나눠놔야한다. 이건 seed를 고정해놔도 random split하면서 바뀜 \n",
    "## 따라서, 동일한 환경에서 실험할 수 있도록 할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =  32 #한 배치당 32개 이미지데이터\n",
    "EPOCHS = 40 # 전체 데이터 셋을 40번 반복\n",
    "lr = 1e-2 # 학습률(learnign rate)\n",
    "\n",
    "csv_file = \"\"\n",
    "root_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from util.data import CustomDataset\n",
    "\n",
    "# 이미지 전처리 및 데이터셋 생성\n",
    "transform = transforms.Compose([ ## Albumentation 사용하는 것으로 변경\n",
    "    transforms.Resize((224, 224)),  # 원하는 크기로 리사이즈\n",
    "    transforms.ToTensor(),           # 텐서로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
    "])\n",
    "\n",
    "## shuffle은 false로 두고\n",
    "## 데이터를 나눌 때 idx를 섞고 csv 파일을 나눠놓을 것 => train, val 비율 클래스별로 9:1\n",
    "## k-fold => begging과 boosting을 찾아보면 좋을 것\n",
    "## k-fold => 하나의 모델을 k번 학습하는 것\n",
    "## 9:1 나눈다고 했을 때 10%를 안쓰고 있고, 마지막 1~2 epoch에서 10%로 학습하고 90%를 검증으로 사용 => 여전히 성능이 validation과 train이 유지 또는 오른다면 일반화되고 있다는 것\n",
    "## + 개선 여지가 있다는 것\n",
    "## 학습해야할 데이터가 남아 있을 때 제출하는건 횟수 낭비다!\n",
    "## 현실에서는 val과 test를 나누진 않음\n",
    "\n",
    "## utils는 torch와 겹칠 수 있으므로 우리 폴더명 utils은 util로 작성하기 \n",
    "## dataloader.py => data.py로 만들고 그 안에 customdataset과 def custom_data_loader로 작성해서 train.ipynb에서는 custom_data_loader만 불러올 수 있도록\n",
    "\n",
    "train_dataset = CustomDataset(\"\", root_dir='./data', transform=transform)\n",
    "val_dataset = CustomDataset(\"\", root_dir='./data', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle = False, # 순서가 암기되는것을 막기위해.\n",
    "                                                    )\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle = False, # 테스트 순서 유지.\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6400, 800),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(800, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_layer1(x)\n",
    "        output = self.conv_layer2(output)\n",
    "        output = self.conv_layer3(output)\n",
    "        output = torch.flatten(output, 1)\n",
    "        output = self.fc_layer1(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test): #이진 분류?\n",
    "\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    #checkpoint_filepath = 'checkpoint.pth'  # 체크포인트 저장 파일 경로\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkihoon090\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Kwak\\Desktop\\School\\네이버부캠\\프로젝트\\ClassificationSketch\\wandb\\run-20240909_183702-9hlcnuwz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kihoon090/my-test-project/runs/9hlcnuwz' target=\"_blank\">absurd-pyramid-1</a></strong> to <a href='https://wandb.ai/kihoon090/my-test-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kihoon090/my-test-project' target=\"_blank\">https://wandb.ai/kihoon090/my-test-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kihoon090/my-test-project/runs/9hlcnuwz' target=\"_blank\">https://wandb.ai/kihoon090/my-test-project/runs/9hlcnuwz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kihoon090/my-test-project/runs/9hlcnuwz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2e9b05264c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'epoches': EPOCHS, 'batch_size': BATCH_SIZE, 'learning_rate': lr}\n",
    "wandb.init(project='my-test-project', config=config)\n",
    "\n",
    "## 시각화 관련해서\n",
    "## 다 찍는게 좋음 \n",
    "## 전체 Loss & Acc\n",
    "## class별 loss & Acc\n",
    "## Learning rate\n",
    "## 데이터가 적은 경우 => 틀린 그림의 idx 뽑아보기\n",
    "## 최근에 acc보단 F1 score를 더 많이 사용하기도 해서 같이 찍어보면 좋음\n",
    "## acc와 F1 score 중 둘 중 하나 더 높은 모델들 끼리 비교했을 때 무엇이 더 일반화 성능이 더 좋을지 모름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Loss: 0.04759 | Acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Loss: 0.04763 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_1.pth\n",
      "Checkpoint updated at epoch 1 and saved as checkpoints\\checkpoint_epoch_1.pth\n",
      "Training on 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.04760 | Acc: 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.04764 | Acc: 0.784\n",
      "Training on 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002: | Loss: 0.04759 | Acc: 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002: | Loss: 0.04763 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_3.pth\n",
      "Checkpoint updated at epoch 3 and saved as checkpoints\\checkpoint_epoch_3.pth\n",
      "Training on 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003: | Loss: 0.04758 | Acc: 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003: | Loss: 0.04760 | Acc: 0.779\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_4.pth\n",
      "Checkpoint updated at epoch 4 and saved as checkpoints\\checkpoint_epoch_4.pth\n",
      "Training on 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004: | Loss: 0.04757 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004: | Loss: 0.04757 | Acc: 0.779\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_5.pth\n",
      "Checkpoint updated at epoch 5 and saved as checkpoints\\checkpoint_epoch_5.pth\n",
      "Training on 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005: | Loss: 0.04759 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005: | Loss: 0.04753 | Acc: 0.779\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_6.pth\n",
      "Checkpoint updated at epoch 6 and saved as checkpoints\\checkpoint_epoch_6.pth\n",
      "Training on 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006: | Loss: 0.04755 | Acc: 0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006: | Loss: 0.04752 | Acc: 0.779\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_7.pth\n",
      "Checkpoint updated at epoch 7 and saved as checkpoints\\checkpoint_epoch_7.pth\n",
      "Training on 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007: | Loss: 0.04755 | Acc: 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007: | Loss: 0.04751 | Acc: 0.779\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_8.pth\n",
      "Checkpoint updated at epoch 8 and saved as checkpoints\\checkpoint_epoch_8.pth\n",
      "Training on 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008: | Loss: 0.04755 | Acc: 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008: | Loss: 0.04749 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_9.pth\n",
      "Checkpoint updated at epoch 9 and saved as checkpoints\\checkpoint_epoch_9.pth\n",
      "Training on 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009: | Loss: 0.04754 | Acc: 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009: | Loss: 0.04748 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_10.pth\n",
      "Checkpoint updated at epoch 10 and saved as checkpoints\\checkpoint_epoch_10.pth\n",
      "Training on 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.04760 | Acc: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.04748 | Acc: 0.784\n",
      "Training on 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011: | Loss: 0.04761 | Acc: 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011: | Loss: 0.04747 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_12.pth\n",
      "Checkpoint updated at epoch 12 and saved as checkpoints\\checkpoint_epoch_12.pth\n",
      "Training on 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012: | Loss: 0.04756 | Acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012: | Loss: 0.04748 | Acc: 0.784\n",
      "Training on 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013: | Loss: 0.04762 | Acc: 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013: | Loss: 0.04746 | Acc: 0.784\n",
      "Checkpoint saved at checkpoints\\checkpoint_epoch_14.pth\n",
      "Checkpoint updated at epoch 14 and saved as checkpoints\\checkpoint_epoch_14.pth\n",
      "Training on 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014: | Loss: 0.04760 | Acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014: | Loss: 0.04749 | Acc: 0.784\n",
      "Training on 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015: | Loss: 0.04758 | Acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015: | Loss: 0.04748 | Acc: 0.784\n",
      "Training on 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016: | Loss: 0.04759 | Acc: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016: | Loss: 0.04749 | Acc: 0.784\n",
      "Training on 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017: | Loss: 0.04757 | Acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017: | Loss: 0.04751 | Acc: 0.784\n",
      "Training on 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018: | Loss: 0.04756 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018: | Loss: 0.04750 | Acc: 0.784\n",
      "Training on 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019: | Loss: 0.04763 | Acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019: | Loss: 0.04750 | Acc: 0.784\n",
      "Training on 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020: | Loss: 0.04759 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020: | Loss: 0.04751 | Acc: 0.784\n",
      "Training on 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021: | Loss: 0.04759 | Acc: 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021: | Loss: 0.04756 | Acc: 0.779\n",
      "Training on 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022: | Loss: 0.04758 | Acc: 0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022: | Loss: 0.04757 | Acc: 0.779\n",
      "Training on 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023: | Loss: 0.04759 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023: | Loss: 0.04758 | Acc: 0.779\n",
      "Training on 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024: | Loss: 0.04757 | Acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024: | Loss: 0.04758 | Acc: 0.779\n",
      "Training on 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025: | Loss: 0.04759 | Acc: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025: | Loss: 0.04759 | Acc: 0.784\n",
      "Training on 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026: | Loss: 0.04758 | Acc: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026: | Loss: 0.04758 | Acc: 0.779\n",
      "Training on 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027: | Loss: 0.04758 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027: | Loss: 0.04760 | Acc: 0.779\n",
      "Training on 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028: | Loss: 0.04760 | Acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028: | Loss: 0.04760 | Acc: 0.779\n",
      "Training on 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029: | Loss: 0.04758 | Acc: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029: | Loss: 0.04756 | Acc: 0.779\n",
      "Training on 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030: | Loss: 0.04758 | Acc: 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030: | Loss: 0.04752 | Acc: 0.784\n",
      "Training on 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:03<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031: | Loss: 0.04758 | Acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031: | Loss: 0.04751 | Acc: 0.784\n",
      "Training on 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032: | Loss: 0.04761 | Acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032: | Loss: 0.04751 | Acc: 0.784\n",
      "Training on 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033: | Loss: 0.04758 | Acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val:  29%|██▊       | 2/7 [00:01<00:03,  1.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m## Validation\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     41\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\Desktop\\School\\네이버부캠\\프로젝트\\ClassificationSketch\\utils\\dataloader.py:47\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 데이터 증강\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 47\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label_idx\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kwak\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from util.checkpoints import save_checkpoint\n",
    "\n",
    "#checkpoint_interval = 5 ## 5에폭마다 체크포인트 저장\n",
    "best_val_loss = float('inf')  # 초기값 무한대\n",
    "\n",
    "# checkpoints 폴더가 없으면 생성\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    ## Training\n",
    "    model.train() ## 학습용\n",
    "    print(f\"Training on {epoch}\")\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    for images, labels in tqdm(train_loader, desc=\"train\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            # size += len(labels)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(images)\n",
    "        \n",
    "        # pred => [0.2, 0.3, 0.5, 0] \n",
    "        # label => [0] or [1] or [2] or [3] \n",
    "        \n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #손실 계산\n",
    "        train_loss += loss.item()\n",
    "        #train_acc += acc.item()\n",
    "        \n",
    "        #정확도 계산\n",
    "        # pred => argmax()\n",
    "        # label  => [0] or [1] \n",
    "        pred = torch.argmax(pred, dim=1) #이건 다중 분류에서 쓰는 건데\n",
    "        #acc = binary_acc(pred, labels) #이거는 이진 분륜디\n",
    "        acc = (pred == labels).sum().item()\n",
    "        train_correct += acc\n",
    "\n",
    "\n",
    "    \n",
    "    #배치별로 평균\n",
    "    #train_loss = train_loss / len(train_dataset) ## data의 개수로 나눠주는게 아님 => 위에 size를 나눠주는 방식을 사용함 or dataload의 매개변수로 droplast가 있는데, batch 안채워진거 제거\n",
    "    #train_acc = train_acc / len(train_dataset)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = train_correct / len(train_loader.dataset)  # 전체 데이터셋에 대한 정확도 계산\n",
    "    print(f'Epoch {epoch+1:03}: | Train Loss: {avg_train_loss:.5f} | Train Acc: {train_acc:.3f}')\n",
    "\n",
    "    #Wandb로 train 손실, 정확도 기록\n",
    "    wandb.log({'Train Accuracy': train_acc, 'Train Loss': avg_train_loss, \"Epoch\": epoch + 1})\n",
    "    \n",
    "\n",
    "    ## Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"val\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            #acc = binary_acc(pred, labels)\n",
    "            val_correct += (pred == labels).sum().item()\n",
    "\n",
    "            #val_acc += acc.item()\n",
    "\n",
    "    #배치별로 평균\n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / len(val_loader.dataset) #ㅈ너체 데이터셋에 대한 정확도\n",
    "    print(f'Epoch {epoch+1:03}: | Val Loss: {val_loss_avg:.5f} | Val Acc: {val_acc:.3f}')\n",
    "    \n",
    "    #wandb로 검증 손실 및 정확도 기록\n",
    "    wandb.log({'Val Accuracy': val_acc, 'Val Loss': val_loss_avg})\n",
    "    ## 매 epooch마다 validation을 계산하는 것이 의미가 없다. 따라서 train loss가 낮아졌을 때 validation을 진행하는 것이 바람직함.\n",
    "    ## 큰 데이터를 가지고 학습을 할 때 특히 학습 시간을 줄 일 때 유용함\n",
    "\n",
    "    # validation이 개선되었을 때만 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_filepath = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')  # 에포크 번호 포함\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_filepath)\n",
    "        print(f\"Checkpoint updated at epoch {epoch + 1} and saved as {checkpoint_filepath}\")\n",
    "\n",
    "# 최종 체크포인트 저장\n",
    "#save_checkpoint(model, optimizer, epoch, train_loss / len(train_loader), 'final_checkpoint.pth')\n",
    "final_checkpoint_filepath = os.path.join(checkpoint_dir, 'final_checkpoint.pth')\n",
    "save_checkpoint(model, optimizer, epoch, avg_train_loss, final_checkpoint_filepath)\n",
    "print(f\"Final checkpoint saved as {final_checkpoint_filepath}\")\n",
    "\n",
    "## excel로 실험 관리\n",
    "## notion or spread sheet\n",
    "\n",
    "## 들어갈 내용\n",
    "## 실험자\n",
    "## 모델에 대한 구체적인 정보 ex) resnet18 resnet19 \n",
    "## transfer learning을 했다면 freezing을 어디까지 했는지, pretrain은 feature를 뽑아내는 것까지 밖에 못한다. => 뒷 단의 정보 ex) MLP(1024 -> 1024 -> 500, init 방법)를 어떻게 뭘 적용했는가\n",
    "## scheduler\n",
    "## Lr\n",
    "## 등등\n",
    "## 실험 결과 작성을 할 때\n",
    "## 짧은 의견을 달아주는 것이 실험 공유가 편함 (붙이는 정보에 포함)\n",
    "## ex) MLP 4층 이상 쌓아보이니 안 좋음\n",
    "## ex) batch norm, \n",
    "## 추가적인 상대방을 배려할 때 사용되는 GPU 메모리양 \n",
    "## gpu를 사용할 때 모델을 한번씩 하는 것보다 같은 동시에 올려서 하는게 더 빠름 \n",
    "\n",
    "## 최소 몇 epoch를 훈련해야겠더라도 알아내야함\n",
    "## seed number\n",
    "## image transformation 바꾼게 있다면 포함\n",
    "\n",
    "## 마지막에 앙상블 \n",
    "## hard voting => 모든 모델이 동일한 가중치\n",
    "## soft voting => 모델이 각각 다른 가중치 => 지표(train loss, val loss, acc)가 필요함\n",
    "\n",
    "## 추가적인 생각\n",
    "## 학습을 하는데 틀리는 데이터는 분명히 존재할 것이다. \n",
    "## validation을 random하게 짜는 것이 좋은게 아닐 수 있다\n",
    "##   => 사람이 봐도 어려운 문제가 있다고 하자, 이런 것을 맞추는 모델이 똑똑한 것인지 생각해볼 것\n",
    "##   => 팀내의 정책을 정해서 쉬운 데이터, 어려운 데이터, 매우 어려운 데이터로 분류해서 validation을 구성하는 것이 좋음\n",
    "##   => 이렇게 해야 데이터의 난이도에 상관 없이 잘 분류하는 모델을 얻을 수 있다.\n",
    "##   => 완전히 데이터가 많을 때에는 random하게 나눠도 문제 없다.\n",
    "\n",
    "## ipynb는 간단한 아이디어를 검증하기 위해서 사용할 수 있음 \n",
    "## 실제 수행은 py 파일에서 하는 것이 맞음 => shell script 자동화 해놓을 수 있음\n",
    "\n",
    "## 기회가 남는다면 하이퍼파라미터 조정도 시도해볼 수 있을 것이다.\n",
    "\n",
    "## residual, normalize, CNN N층 => N층 자동 생성\n",
    "\n",
    "## activaiton.py은 어차피 활성화함수를 구현할게 아니기 때문에 필요 없음\n",
    "\n",
    "## paperswithcode에서 모델 찾아보고 적용해볼 것\n",
    "\n",
    "# 자료형 명시\n",
    "# def __len__(self) -> int: ## 이런걸 전부 포함하도록 하는게 좋음\n",
    "#     # 데이터 개수 반환 함수\n",
    "#     return len(self.image_paths)\n",
    "\n",
    "## baseline code를 어떻게 작성했는지 많이 찾아보고 분석해볼 것\n",
    "\n",
    "## 목표\n",
    "## 호철이형 : 전체적인 시도 => 다양한 시도\n",
    "## 나 : 모델링 + baseline 개선 => 모델링 분석 + baseline code 분석 및 개선\n",
    "## 재환이형 : 모델 관련해서 어떻게 구현되어 있는지 분석해서 공유\n",
    "## 유향이 : 큰 모델의 transfer learning, 모델 밑바닥 빌딩 => 데이터 feature 추출과 layer size를 제대로 파악 <= 감각적으로 이해하는 것도 중요하고 다른 팀원들에게 정보를 공유하면 좋을 듯\n",
    "## 종민이형 : 전체적인 흐름 이해 + data & data augmentation technique 실험 => 시각화부터 시작해서 데이터를 살펴보는 것부터 시작. augmentation이 많아질 수록 직접 시각화하여 어떤 결과인지 확인해보기\n",
    "##           data augmentation 적용했을 때 무엇이 잘 되었고 잘 되지 못했는지 파악하기, library에 없는 augmentation을 직접 구현해볼 수 도 있을 것\n",
    "##           mixup을 시도할 때 labeling을 나누는 것 비율\n",
    "## 소윤이 : 개발의 흐름 이해 + 서버 이용 방법 + 다른 사람 코드 분석해서 추가적인 insight 도출 <= 이건 팀원 모두가 해야함, 즉 모두가 답이 나와야함 <= 포폴에 넣었을 때 면접가서 질문이 들어오면 다 방어할 수 있음\n",
    "## 없는 내용 : optimization(adam, Radam 등등)/scheduler(cosine scheduler) 부분을 소윤이가 담당해보는 것이 좋아보임 => 잘 모르는 경우가 많음\n",
    "\n",
    "## 프젝 기간 동안 성장을 해야한다. 따라서 계획이 그만큼 중요한 것. \n",
    "## 이론 부분은 언제든지 되돌아가서 볼 수 있다.\n",
    "## 개개인의 성장을 느끼는 것이 중요.\n",
    "## 개개인의 시너지를 잘 이용했으면 좋겠다. \n",
    "\n",
    "## data augmentation에 대해서\n",
    "## data transform이라고 부르는 것이 더 맞음\n",
    "## 증강된 데이터로만 학습해도 문제가 없는 것인가? \n",
    "# => offline(새로운 이미지를 생성) augmentation(transform을 적용했을 때 성능이 오르는게 확정적일 때 사용)과 online(확률적 적용, 불러올 때 적용) augmentation으로 나눈다.\n",
    " \n",
    "## RGB와 Grayscale\n",
    "## RGB가 superset이므로 일반적으로 Grayscale보다 RGB로 하는게 맞지만, 이는 확인해보는게 좋음\n",
    "\n",
    "## 프로젝트 역할 구분\n",
    "## 크게 구분하면 => 위에 나왔던 내용\n",
    "\n",
    "## outlier가 치명적이면 버리고 필요하면 사용하는게 맞다.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
